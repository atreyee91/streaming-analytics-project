ğŸ—ï¸ Project Architecture Overview
A real-time e-commerce clickstream analytics pipeline - essentially a mini version of what companies like Amazon, Shopify, or Netflix use to analyze user behavior in real-time.

ğŸ“Š Component by Component
1. Event Generator (src/event_generator.py)
What it does: Simulates realistic user behavior on an e-commerce website
Key features:

Generates 4 event types: page_view, add_to_cart, purchase, search
Creates realistic user journeys (sessions with conversion funnels)
Injects anomalies (bot traffic) for testing detection
Writes events to JSON files every 10 seconds (simulates real-time data stream)

Output: Files in data/raw/events/ like:
events_20250207_172501.json
events_20250207_172511.json
...
Real-world equivalent: This simulates data coming from website tracking (Google Analytics, Segment, Snowplow)

2. Stream Processor (src/stream_processor.py)
What it does: Processes events in real-time using PySpark Structured Streaming
Architecture - Medallion Pattern (Bronze/Silver/Gold):
Raw JSON Files
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BRONZE LAYER (data/bronze/events)  â”‚
â”‚  - Raw ingestion                    â”‚
â”‚  - Minimal transformation           â”‚
â”‚  - Add ingestion timestamp          â”‚
â”‚  - Partition by date                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SILVER LAYER (data/silver/events)  â”‚
â”‚  - Cleansed & validated             â”‚
â”‚  - Deduplicated by event_id         â”‚
â”‚  - Type conversions                 â”‚
â”‚  - Add derived columns (date, hour) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOLD LAYER (Business Metrics)      â”‚
â”‚                                     â”‚
â”‚  1. Anomalies (data/gold/anomalies) â”‚
â”‚     - Detects bot traffic           â”‚
â”‚     - Users with >50 events/min     â”‚
â”‚                                     â”‚
â”‚  2. Revenue (data/gold/revenue)     â”‚
â”‚     - 5-min windows                 â”‚
â”‚     - Revenue by product            â”‚
â”‚     - Purchase counts               â”‚
â”‚                                     â”‚
â”‚  3. Conversion (data/gold/conv...)  â”‚
â”‚     - 10-min session windows        â”‚
â”‚     - Funnel metrics                â”‚
â”‚     - Conversion rates              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Key Streaming Concepts Used:

Watermarking: Handles late-arriving data (events that arrive out of order)
Windows: Time-based aggregations (tumbling windows of 1min, 5min, 10min)
Checkpointing: Ensures exactly-once processing (can resume after failures)
Delta Lake: ACID transactions, versioning, time travel

Real-world equivalent: This is like Spark Streaming jobs at Uber, Netflix, or LinkedIn processing clickstreams

3. Inspection & Testing Scripts
src/test_generator.py

Runs the event generator for testing
Generates 60 seconds of data
Prints statistics

src/inspect_events.py

Validates raw JSON events
Checks data quality
Shows event distribution

src/run_pipeline.py

Orchestrates the entire streaming pipeline
Manages multiple streaming queries
Handles graceful shutdown

src/inspect_delta_tables.py (you're about to create this)

Reads Delta tables
Shows processed results
Validates pipeline output


ğŸ¯ Data Flow Example
How a single user journey flows through the system:

1. User arrives on website (T=0s)
json{
  "event_type": "page_view",
  "user_id": "user_1234",
  "session_id": "sess_abc",
  "product_id": "prod_42",
  "timestamp": "2025-02-07T17:25:01"
}
â†“ Written to data/raw/events/events_20250207_172501.json
2. Stream Processor reads file
â†“ Bronze Layer: Stores exactly as-is + adds ingestion_timestamp
3. User adds to cart (T=15s)
json{
  "event_type": "add_to_cart",
  "user_id": "user_1234",
  "session_id": "sess_abc",
  "product_id": "prod_42",
  "timestamp": "2025-02-07T17:25:16"
}
â†“ Silver Layer: Deduplicates, validates, adds is_purchase=False
4. User completes purchase (T=30s)
json{
  "event_type": "purchase",
  "user_id": "user_1234",
  "product_id": "prod_42",
  "price": 199.99,
  "quantity": 1,
  "timestamp": "2025-02-07T17:25:31"
}
```

**5. Gold Layer aggregations trigger:**

**Revenue Table** (5-min window):
```
window: [17:25:00 - 17:30:00]
product_id: prod_42
total_revenue: 199.99
purchase_count: 1
```

**Conversion Funnel** (10-min window):
```
session_id: sess_abc
page_views: 1
add_to_carts: 1
purchases: 1
conversion_rate: 100%  (1/1)

ğŸ” Why This Medallion Architecture?
Bronze (Raw)

Purpose: Audit trail, can always reprocess
Benefit: If bugs in transformation logic, you can fix and replay

Silver (Cleansed)

Purpose: Single source of truth for clean data
Benefit: Analysts don't deal with dirty data

Gold (Business Metrics)

Purpose: Pre-aggregated for performance
Benefit: Dashboards query Gold (fast), not raw events (slow)


ğŸ’¡ Key Technologies & Concepts

âœ… Modern Stack: PySpark, Delta Lake (used at top tech companies)
âœ… Real-Time: Streaming, not batch
âœ… Scalable Design: Partitioning, windowing, checkpointing
âœ… Production Patterns: Bronze/Silver/Gold, error handling, monitoring
âœ… Anomaly Detection: Shows ML/analytics thinking
âœ… End-to-End: Data generation â†’ Processing â†’ Storage â†’ (Next: Visualization)

ğŸš€ What's Next :
1. Real-Time Dashboard (Streamlit)

Live metrics updating every 5 seconds
Charts for revenue trends
Anomaly alerts
Conversion funnel visualization

2. Advanced Features (Optional)

Replace file-based streaming with Kafka
Add ML-based fraud detection
Implement data quality monitoring (Great Expectations)
Deploy to cloud (AWS/GCP)
